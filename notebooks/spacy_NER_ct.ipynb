{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning Spacy\n",
    "\n",
    "We are finetuning SpaCy model since it is already made for NER. This notebook is based on this website : https://www.freecodecamp.org/news/how-to-fine-tune-spacy-for-nlp-use-cases/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import spacy\n",
    "import re\n",
    "import random\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.util import filter_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = r\"..\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/kiruthigaa/ner-model-train-test-using-spacy-label-studio\n",
    "\n",
    "def import_label_studio_data(filename):\n",
    "    \"\"\"\n",
    "    This function imports the data from label-studio and converts it into the format required by spacy.\n",
    "    \"\"\"\n",
    "    TRAIN_DATA = []\n",
    "    \n",
    "    with open(filename,'rb') as fp:\n",
    "        training_data = json.load(fp)\n",
    "    for text in training_data:\n",
    "        entities = []\n",
    "        info = text.get('text')\n",
    "        if text.get('label') is not None:\n",
    "            list_ = []\n",
    "            for label in text.get('label'):\n",
    "                list_.append([label.get('start'), label.get('end')])\n",
    "            a = np.array(list_)\n",
    "            overlap_ind =[]\n",
    "            for i in range(0,len(a[:,0])):\n",
    "                a_comp = a[i]\n",
    "                x = np.delete(a, (i), axis=0)\n",
    "                overlap_flag = any([a_comp[0] in range(j[0], j[1]+1) for j in x])\n",
    "                if overlap_flag:\n",
    "                    overlap_ind.append(i)\n",
    "                    \n",
    "            for ind, label in enumerate(text.get('label')):\n",
    "                if ind in overlap_ind:\n",
    "                    iop=0\n",
    "                else:\n",
    "                    if label.get('labels') is not None:\n",
    "                        entities.append((label.get('start'), label.get('end') ,label.get('labels')[0]))\n",
    "        TRAIN_DATA.append((info, {\"entities\" : entities}))\n",
    "    return TRAIN_DATA\n",
    "\n",
    "# found on https://stackoverflow.com/questions/56642816/valueerror-e024-could-not-find-an-optimal-move-to-supervise-the-parser\n",
    "\n",
    "\n",
    "def trim_entity_spans(data: list) -> list:\n",
    "    \"\"\"Removes leading and trailing white spaces from entity spans.\n",
    "\n",
    "    Args:\n",
    "        data (list): The data to be cleaned in spaCy JSON format.\n",
    "\n",
    "    Returns:\n",
    "        list: The cleaned data.\n",
    "    \"\"\"\n",
    "    invalid_span_tokens = re.compile(r'\\s')\n",
    "\n",
    "    cleaned_data = []\n",
    "    for text, annotations in data:\n",
    "        entities = annotations['entities']\n",
    "        valid_entities = []\n",
    "        for start, end, label in entities:\n",
    "            valid_start = start\n",
    "            valid_end = end\n",
    "            while valid_start < len(text) and invalid_span_tokens.match(\n",
    "                    text[valid_start]):\n",
    "                valid_start += 1\n",
    "            while valid_end > 1 and invalid_span_tokens.match(\n",
    "                    text[valid_end - 1]):\n",
    "                valid_end -= 1\n",
    "            valid_entities.append([valid_start, valid_end, label])\n",
    "        cleaned_data.append([text, {'entities': valid_entities}])\n",
    "\n",
    "    return cleaned_data\n",
    "\n",
    "def split_dataset(dataset, train_percentage, val_percentage, seed=None):\n",
    "    \"\"\"\n",
    "    Split a dataset into training, validation, and test sets based on the provided percentages.\n",
    "    \n",
    "    Args:\n",
    "    dataset (list): A list of tuples, where each tuple contains a text and its annotations.\n",
    "    train_percentage (float): The percentage of data to be allocated for training.\n",
    "    val_percentage (float): The percentage of data to be allocated for validation.\n",
    "    seed (int) [Default: None]: Seed value for randomization.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: A tuple containing the training dataset, validation dataset, and test dataset.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    \n",
    "    random.shuffle(dataset)\n",
    "    \n",
    "    train_index = int(len(dataset) * train_percentage)\n",
    "    val_index = int(len(dataset) * (train_percentage + val_percentage))\n",
    "    \n",
    "    train_data = dataset[:train_index]\n",
    "    val_data = dataset[train_index:val_index]\n",
    "    test_data = dataset[val_index:]\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "def create_spacy_model(data,name_file_model):\n",
    "    # This load a blank pipeline in spacy model, we will model it to our needs : https://spacy.io/api/top-level\n",
    "    nlp = spacy.blank(\"fr\")\n",
    "    doc_bin = DocBin()  # Instantiate the DocBin class\n",
    "    \n",
    "    for item in data:\n",
    "        text = item[0]\n",
    "        labels = item[1][\"entities\"]\n",
    "        doc = nlp.make_doc(text) \n",
    "        ents = []\n",
    "        for start, end, label in labels:\n",
    "            span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "            if span is None:\n",
    "                continue\n",
    "                #print(\"Skipping entity\")\n",
    "            else:\n",
    "                ents.append(span)\n",
    "        filtered_ents = filter_spans(ents)\n",
    "        doc.ents = filtered_ents\n",
    "        doc_bin.add(doc)\n",
    "    \n",
    "    doc_bin.to_disk(os.path.join(MODEL_PATH,name_file_model))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune of SpaCy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first code block we will import all the necessary library to finetune Spacy and load a blank pipeline as well as our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = ['OUV', 'INT', 'CAD', 'NOUV', 'NCAD', 'AG', 'AI', 'TOUS', 'AG OUV', 'AG INT', 'AG CAD', 'AI OUV', 'AI INT', 'AI CAD', 'NOUV AG', 'NCAD AG', 'NOUV AI', 'NCAD AI', 'ATOT',\\\n",
    "        'ATOT OUV', 'ATOT INT', 'ATOT CAD', 'PPV', 'PPVm', 'DATE']\n",
    "\n",
    "for LABEL in all : \n",
    "    print(f\"Training LABEL = {LABEL}\")\n",
    "    MODEL_PATH=os.path.join(ROOT_PATH,\"models/NER\",f\"{LABEL.replace(' ','_')}/\")\n",
    "    !mkdir -p {MODEL_PATH} 2> /dev/null\n",
    "\n",
    "    data = import_label_studio_data(os.path.join(ROOT_PATH, r\"data/raw/data449.json\"))\n",
    "    TRAIN_DATAS = trim_entity_spans(data)\n",
    "    train_data, val_data, test_data = split_dataset(TRAIN_DATAS, 0.7, 0.2)\n",
    "    \n",
    "    print(f\"Training data size: {len(train_data)}\")\n",
    "    print(f\"Validation data size: {len(val_data)}\")\n",
    "    print(f\"Test data size: {len(test_data)}\")\n",
    "    \n",
    "    # We create the document to finetune spacy model\n",
    "    \n",
    "    create_spacy_model(train_data,\"train.spacy\")\n",
    "    create_spacy_model(val_data,\"val.spacy\")\n",
    "    create_spacy_model(test_data,\"test.spacy\")\n",
    "\n",
    "    !python -m spacy init config  --lang fr --pipeline ner {MODEL_PATH}\\config.cfg\n",
    "    !python -m spacy train {MODEL_PATH}\\config.cfg --output {MODEL_PATH} --paths.train {MODEL_PATH}\\train.spacy --paths.dev {MODEL_PATH}\\val.spacy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
