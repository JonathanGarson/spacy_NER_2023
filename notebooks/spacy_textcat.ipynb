{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy Text Categorizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective here will be to build a french langage categorizer suited to distinguish different kind of wage agreement components."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some inspiring sources:\n",
    "- https://medium.com/@johnidouglasmarangon/building-a-text-classification-model-with-spacy-3-x-57e59fa50547\n",
    "- https://www.machinelearningplus.com/nlp/custom-text-classification-spacy/\n",
    "- https://www.width.ai/post/spacy-text-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = r\"C:\\Users\\garsonj\\Desktop\\spacy_finetuning\\spacy_files\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to adapt the data from label studio to the required format for spacy TextCategorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_label_studio_data(filename, target_labels):\n",
    "    \"\"\"\n",
    "    This function imports the data from Label Studio JSON file and returns the data in the format required for training.\n",
    "    It also allows to select specific labels to train the model on with the \"target_labels\" argument.\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(target_labels, list):\n",
    "        raise ValueError(\"The 'target_labels' argument must be a list of strings.\")\n",
    "\n",
    "    TRAIN_DATA = []  # Initialize TRAIN_DATA\n",
    "    \n",
    "    with open(filename, 'rb') as fp:\n",
    "        training_data = json.load(fp)\n",
    "    for text in training_data:\n",
    "        entities = []\n",
    "        info = text.get('text')\n",
    "        entities = []\n",
    "        if text.get('label') is not None:\n",
    "            list_ = []\n",
    "            for label in text.get('label'):\n",
    "                list_.append([label.get('start'), label.get('end')])\n",
    "            a = np.array(list_)\n",
    "            overlap_ind = []\n",
    "            for i in range(0, len(a[:, 0])):\n",
    "                a_comp = a[i]\n",
    "                x = np.delete(a, (i), axis=0)\n",
    "                overlap_flag = any([a_comp[0] in range(j[0], j[1] + 1) for j in x])\n",
    "                if overlap_flag:\n",
    "                    overlap_ind.append(i)\n",
    "\n",
    "            for ind, label in enumerate(text.get('label')):\n",
    "                if ind in overlap_ind:\n",
    "                    iop = 0\n",
    "                else:\n",
    "                    if any(target in label.get('labels') for target in target_labels):\n",
    "                        entities.append((label.get('start'), label.get('end'), label.get('labels')[0]))\n",
    "        \n",
    "        if entities:  # Proceed only if there are non-empty entities\n",
    "            TRAIN_DATA.append((info, {\"entities\": entities}))\n",
    "\n",
    "    return TRAIN_DATA\n",
    "\n",
    "all = ['OUV', 'INT', 'CAD', 'NOUV', 'NCAD', 'AG', 'AI', 'TOUS', 'AG OUV', 'AG INT', 'AG CAD', 'AI OUV', 'AI INT', 'AI CAD', 'NOUV AG', 'NCAD AG', 'NOUV AI', 'NCAD AI', 'ATOT',\\\n",
    "        'ATOT OUV', 'ATOT INT', 'ATOT CAD', 'PPV', 'PPVm', 'DATE']\n",
    "\n",
    "# Call the function with the filename and a list of target labels\n",
    "target_labels = ['OUV', 'INT', 'CAD', 'NOUV', 'NCAD', 'AG', 'AI', 'TOUS', 'AG OUV', 'AG INT', 'AG CAD', 'AI OUV', 'AI INT', 'AI CAD', 'NOUV AG', 'NCAD AG', 'NOUV AI', 'NCAD AI', 'ATOT',\\\n",
    "        'ATOT OUV', 'ATOT INT', 'ATOT CAD', 'PPV', 'PPVm', 'DATE'] # Add your target labels here\n",
    "\n",
    "data = import_label_studio_data(os.path.join(ROOT_PATH, r\"data\\training_json\\data449.json\"), target_labels)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def spacy_to_dataframe(data):\n",
    "    \"\"\"\n",
    "    This function takes the data in the format returned by the import_label_studio_data function and returns a pandas dataframe of two columns: text and label.\n",
    "\n",
    "    Args:\n",
    "        data: The data in the format returned by the import_label_studio_data function.\n",
    "\n",
    "    Returns:\n",
    "        A pandas dataframe of two columns: text and label.\n",
    "    \"\"\"\n",
    "    text_data = [text for text, _ in data]\n",
    "    labels = [label for _, label in data]\n",
    "\n",
    "    df = pd.DataFrame({'text': text_data, 'label': labels})\n",
    "    return df\n",
    "\n",
    "df = spacy_to_dataframe(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_label(df):\n",
    "    \"\"\"\n",
    "    This function creates a dummy variable for the target label.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The DataFrame containing the text and label columns.\n",
    "    \"\"\"\n",
    "    # Create a new column called \"label_dummy\" and initialize with zeros\n",
    "    df[\"label_dummy\"] = 0\n",
    "\n",
    "    # Iterate through each row in the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        labels = row[\"label\"][\"entities\"]  # Access the entities list in the tuple\n",
    "        for label in labels:\n",
    "            target = label[2]\n",
    "            if target == \"OUV\":\n",
    "                df.at[index, \"label_dummy\"] = 1  # Set the value to 1 for the current row\n",
    "\n",
    "    # Print the DataFrame to verify the changes\n",
    "    print(df[\"label_dummy\"].value_counts())\n",
    "    return df\n",
    "\n",
    "df = dummy_label(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning function : \n",
    "def clean_dataset(data):\n",
    "    \"\"\"\n",
    "    This function cleans the dataset by removing rows with missing values and dropping the \"label\" column.\n",
    "    It also renames the \"label_dummy\" column to \"label\".\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): The DataFrame containing the text, label and label_dummy columns.\n",
    "    \"\"\"\n",
    "    data.dropna(axis=0, how='any', inplace=True)\n",
    "    # Now we can drop the \"label\" column and rename the \"label_dummy\" column to \"label\"\n",
    "    if 'label_dummy' in data.columns:\n",
    "        data.drop(\"label\", axis=1, inplace=True)\n",
    "        data.rename(columns={\"label_dummy\": \"label\"}, inplace=True)\n",
    "    else:\n",
    "        pass\n",
    "    print(data.head())\n",
    "    return data\n",
    "\n",
    "df = clean_dataset(df)\n",
    "df.to_csv(os.path.join(ROOT_PATH, r\"\\data\\training_csv\\data449_cleaned.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We inverse the label to have 0 for PPV and 1 for the rest\n",
    "def inverse_label(data):\n",
    "    \"\"\"\n",
    "    This function inverses the label column.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): The DataFrame containing the text and label columns.\n",
    "    \"\"\"\n",
    "    data[\"label\"] = data[\"label\"].apply(lambda x: 0 if x == 1 else 1)\n",
    "    print(data[\"label\"].value_counts())\n",
    "    return data\n",
    "\n",
    "# df = inverse_label(df)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "dataset = list(df[[\"text\", \"label\"]].sample(frac=1).itertuples(index=False, name=None))\n",
    "dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our dataset is cleaned, we can split it into 3: the train, the validation and the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def split_data(data, train_ratio=0.75, val_ratio=0.15, test_ratio=0.10, random_seed=None):\n",
    "    \"\"\"\n",
    "    Split a dataset into training, validation, and test sets.\n",
    "\n",
    "    Parameters:\n",
    "    - data: The dataset to be split.\n",
    "    - train_ratio: The ratio of data to be allocated to the training set (default: 0.75).\n",
    "    - val_ratio: The ratio of data to be allocated to the validation set (default: 0.15).\n",
    "    - test_ratio: The ratio of data to be allocated to the test set (default: 0.10).\n",
    "    - random_seed: Seed for the random shuffling (default: None, which results in non-reproducible shuffling).\n",
    "\n",
    "    Returns:\n",
    "    - A tuple containing three sets: (train_set, val_set, test_set)\n",
    "    \"\"\"\n",
    "    # Calculate the total size of the dataset\n",
    "    total_size = len(data)\n",
    "    \n",
    "    # Calculate the sizes of each split\n",
    "    train_size = int(total_size * train_ratio)\n",
    "    val_size = int(total_size * val_ratio)\n",
    "    test_size = total_size - train_size - val_size\n",
    "    \n",
    "    # Set the random seed if provided\n",
    "    if random_seed is not None:\n",
    "        np.random.seed(random_seed)\n",
    "    \n",
    "    # Shuffle the data\n",
    "    shuffled_data = np.random.permutation(data)\n",
    "    shuffled_data =list(map(lambda x:(str(x[0]),int(x[1])),shuffled_data))\n",
    "\n",
    "    # Split the data into three sets\n",
    "    train_set = shuffled_data[:train_size]\n",
    "    val_set = shuffled_data[train_size:train_size + val_size]\n",
    "    test_set = shuffled_data[train_size + val_size:]\n",
    "    \n",
    "    # Print the size of each set\n",
    "    print(\"Training set size:\", len(train_set))\n",
    "    print(\"Validation set size:\", len(val_set))\n",
    "    print(\"Test set size:\", len(test_set))\n",
    "\n",
    "    return train_set, val_set, test_set\n",
    "\n",
    "# Example usage:\n",
    "train_data, val_data, test_data = split_data(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy for training need binary files (doc) so we need to convert the data to this format"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build the model to obtain a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# We create an empty model\n",
    "nlp = spacy.blank(\"fr\")\n",
    "\n",
    "# We add the text classifier to the pipeline\n",
    "# textcat = nlp.add_pipe(\"tok2vec\")\n",
    "# textcat = nlp.add_pipe(\"textcat\")\n",
    "# textcat.add_label(\"PPV\")\n",
    "# textcat.add_label(\"NPPV\")\n",
    "\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin\n",
    "\n",
    "def convert(data, outfile):\n",
    "    db = spacy.tokens.DocBin()\n",
    "\n",
    "    for doc, label in nlp.pipe(data, as_tuples=True):\n",
    "\n",
    "        doc.cats[\"PPV\"] = label == 0\n",
    "        doc.cats[\"NPPV\"] = label == 1\n",
    "     \n",
    "        db.add(doc)\n",
    "    \n",
    "    db.to_disk(outfile)\n",
    "    print(\"Data saved to:\", outfile)\n",
    "\n",
    "convert(train_data, os.path.join(ROOT_PATH, r\"model\\classifyer\\PPV\\train.spacy\"))\n",
    "convert(val_data, os.path.join(ROOT_PATH, r\"model\\classifyer\\PPV\\val.spacy\"))\n",
    "convert(test_data, os.path.join(ROOT_PATH, r\"model\\classifyer\\PPV\\test.spacy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy init config --lang fr --pipeline textcat --optimize efficiency --force C:\\Users\\garsonj\\Desktop\\spacy_finetuning\\spacy_files\\model\\classifyer\\PPV\\config.cfg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy train C:\\Users\\garsonj\\Desktop\\spacy_finetuning\\spacy_files\\model\\classifyer\\OUV\\config.cfg --paths.train C:\\Users\\garsonj\\Desktop\\spacy_finetuning\\spacy_files\\model\\classifyer\\OUV\\train.spacy  --paths.dev C:\\Users\\garsonj\\Desktop\\spacy_finetuning\\spacy_files\\model\\classifyer\\OUV\\val.spacy --output C:\\Users\\garsonj\\Desktop\\spacy_finetuning\\spacy_files\\model\\classifyer\\OUV --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy evaluate C:\\Users\\garsonj\\Desktop\\spacy_finetuning\\spacy_files\\model\\classifyer\\OUV\\model-best C:\\Users\\garsonj\\Desktop\\spacy_finetuning\\spacy_files\\model\\classifyer\\OUV\\test.spacy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"Le contrat ne contient pas de prime de partage de la valeur ajoutée\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(os.path.join(ROOT_PATH, r\"model\\classifyer\\PPV\\model-best\"))\n",
    "\n",
    "for text in texts:\n",
    "    doc = nlp(text)\n",
    "    print(doc.cats,  \"-\",  text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstration: on teste la capacité de tri sur un ensemble de documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# Load the model\n",
    "nlp = spacy.load(os.path.join(ROOT_PATH, r\"model\\classifyer\\PPV\\model-best\"))\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(os.path.join(ROOT_PATH, r\"data\\training_csv\\data449_cleaned.csv\"))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data[\"text\"].tolist()\n",
    "\n",
    "for text in texts:\n",
    "    doc = nlp(text)\n",
    "    print(doc.cats,  \"-\",  text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppv_list = []\n",
    "nppv_list = []\n",
    "\n",
    "for text in texts:\n",
    "    doc = nlp(text)\n",
    "    if doc.cats.get(\"PPV\", 0.0) > doc.cats.get(\"NPPV\", 0.0):\n",
    "        ppv_list.append(text)\n",
    "    else:\n",
    "        nppv_list.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ppv = pd.DataFrame(ppv_list, columns=[\"PPV\"])\n",
    "df_nppv = pd.DataFrame(nppv_list, columns=[\"NPPV\"])\n",
    "\n",
    "df_data = pd.concat([df_ppv, df_nppv], axis=1)\n",
    "df_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\garsonj\\Desktop\\spacy_finetuning\\spacy_files\\script\")\n",
    "from confusion_matrix_spacy_def import print_statistics, import_label_studio_data, spacy_to_dataframe, dummy_label_true, clean_dataset, dummy_label_pred\n",
    "\n",
    "true_data = import_label_studio_data(os.path.join(ROOT_PATH, r\"data\\training_json\\data449.json\"), target_labels)\n",
    "df_true = spacy_to_dataframe(true_data)\n",
    "df_true = dummy_label_true(df_true)\n",
    "df_true = clean_dataset(df_true)\n",
    "\n",
    "# for the predicted json\n",
    "with open(r\"\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data_pred = json.load(f)\n",
    "\n",
    "df_pred = spacy_to_dataframe(data_pred)\n",
    "df_pred = dummy_label_pred(df_pred)\n",
    "df_pred = clean_dataset(df_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy_finetuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
