{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning Spacy\n",
    "\n",
    "We are finetuning SpaCy model since it is already made for NER. This notebook is based on this website : https://www.freecodecamp.org/news/how-to-fine-tune-spacy-for-nlp-use-cases/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = r\"C:\\Users\\garsonj\\Desktop\\spacy_finetuning\\spacy_files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/kiruthigaa/ner-model-train-test-using-spacy-label-studio\n",
    "\n",
    "def import_label_studio_data(filename):\n",
    "    \"\"\"\n",
    "    This function imports the data from label-studio and converts it into the format required by spacy.\n",
    "    \"\"\"\n",
    "    TRAIN_DATA = []\n",
    "    \n",
    "    with open(filename,'rb') as fp:\n",
    "        training_data = json.load(fp)\n",
    "    for text in training_data:\n",
    "        entities = []\n",
    "        info = text.get('text')\n",
    "        if text.get('label') is not None:\n",
    "            list_ = []\n",
    "            for label in text.get('label'):\n",
    "                list_.append([label.get('start'), label.get('end')])\n",
    "            a = np.array(list_)\n",
    "            overlap_ind =[]\n",
    "            for i in range(0,len(a[:,0])):\n",
    "                a_comp = a[i]\n",
    "                x = np.delete(a, (i), axis=0)\n",
    "                overlap_flag = any([a_comp[0] in range(j[0], j[1]+1) for j in x])\n",
    "                if overlap_flag:\n",
    "                    overlap_ind.append(i)\n",
    "                    \n",
    "            for ind, label in enumerate(text.get('label')):\n",
    "                if ind in overlap_ind:\n",
    "                    iop=0\n",
    "                else:\n",
    "                    if label.get('labels') is not None:\n",
    "                        entities.append((label.get('start'), label.get('end') ,label.get('labels')[0]))\n",
    "        TRAIN_DATA.append((info, {\"entities\" : entities}))\n",
    "    return TRAIN_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = import_label_studio_data(os.path.join(ROOT_PATH, r\"data/training_json/data449.json\"))\n",
    "data[0:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "This alternative function is here to select only a specific label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def import_label_studio_data(filename, target_label):\n",
    "    \"\"\"\n",
    "    This function imports the data from Label Studio JSON file and returns the data in the format required for training.\n",
    "    It also allows to select a specific label to train the model on with the \"target_label\" argument.\n",
    "    \"\"\"\n",
    "\n",
    "    TRAIN_DATA = []  # Initialize TRAIN_DATA \n",
    "    \n",
    "    with open(filename, 'rb') as fp:\n",
    "        training_data = json.load(fp)\n",
    "    for text in training_data:\n",
    "        entities = []\n",
    "        info = text.get('text')\n",
    "        entities = []\n",
    "        if text.get('label') is not None:\n",
    "            list_ = []\n",
    "            for label in text.get('label'):\n",
    "                list_.append([label.get('start'), label.get('end')])\n",
    "            a = np.array(list_)\n",
    "            overlap_ind = []\n",
    "            for i in range(0, len(a[:, 0])):\n",
    "                a_comp = a[i]\n",
    "                x = np.delete(a, (i), axis=0)\n",
    "                overlap_flag = any([a_comp[0] in range(j[0], j[1] + 1) for j in x])\n",
    "                if overlap_flag:\n",
    "                    overlap_ind.append(i)\n",
    "\n",
    "            for ind, label in enumerate(text.get('label')):\n",
    "                if ind in overlap_ind:\n",
    "                    iop = 0\n",
    "                else:\n",
    "                    if label.get('labels') == [target_label]:\n",
    "                        entities.append((label.get('start'), label.get('end'), label.get('labels')[0]))\n",
    "        \n",
    "        if entities:  # Proceed only if there are non-empty entities\n",
    "            TRAIN_DATA.append((info, {\"entities\": entities}))\n",
    "\n",
    "    return TRAIN_DATA\n",
    "\n",
    "# Call the function with the filename\n",
    "data = import_label_studio_data(os.path.join(ROOT_PATH, r\"\\data\\training_json\\data360.json\"), \"OUV\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def import_label_studio_data(filename, target_labels):\n",
    "    \"\"\"\n",
    "    This function imports the data from Label Studio JSON file and returns the data in the format required for training.\n",
    "    It also allows to select specific labels to train the model on with the \"target_labels\" argument.\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(target_labels, list):\n",
    "        raise ValueError(\"The 'target_labels' argument must be a list of strings.\")\n",
    "\n",
    "    TRAIN_DATA = []  # Initialize TRAIN_DATA\n",
    "    \n",
    "    with open(filename, 'rb') as fp:\n",
    "        training_data = json.load(fp)\n",
    "    for text in training_data:\n",
    "        entities = []\n",
    "        info = text.get('text')\n",
    "        entities = []\n",
    "        if text.get('label') is not None:\n",
    "            list_ = []\n",
    "            for label in text.get('label'):\n",
    "                list_.append([label.get('start'), label.get('end')])\n",
    "            a = np.array(list_)\n",
    "            overlap_ind = []\n",
    "            for i in range(0, len(a[:, 0])):\n",
    "                a_comp = a[i]\n",
    "                x = np.delete(a, (i), axis=0)\n",
    "                overlap_flag = any([a_comp[0] in range(j[0], j[1] + 1) for j in x])\n",
    "                if overlap_flag:\n",
    "                    overlap_ind.append(i)\n",
    "\n",
    "            for ind, label in enumerate(text.get('label')):\n",
    "                if ind in overlap_ind:\n",
    "                    iop = 0\n",
    "                else:\n",
    "                    if any(target in label.get('labels') for target in target_labels):\n",
    "                        entities.append((label.get('start'), label.get('end'), label.get('labels')[0]))\n",
    "        \n",
    "        if entities:  # Proceed only if there are non-empty entities\n",
    "            TRAIN_DATA.append((info, {\"entities\": entities}))\n",
    "\n",
    "    return TRAIN_DATA\n",
    "\n",
    "# Call the function with the filename and a list of target labels\n",
    "target_labels = ['PPV']  # Add your target labels here\n",
    "\n",
    "# all = ['OUV', 'INT', 'CAD', 'NOUV', 'NCAD', 'AG', 'AI', 'TOUS', 'AG OUV', 'AG INT', 'AG CAD', 'AI OUV', 'AI INT', 'AI CAD', 'NOUV AG', 'NCAD AG', 'NOUV AI', 'NCAD AI', 'ATOT',\\\n",
    "#        'ATOT OUV', 'ATOT INT', 'ATOT CAD', 'ATOT NOUV', 'PPV', 'PPVm', 'DATE']\n",
    "\n",
    "data = import_label_studio_data(os.path.join(ROOT_PATH,r\"data\\training_json\\data449.json\"), target_labels)\n",
    "data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune of SpaCy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first code block we will import all the necessary library to finetune Spacy and load a blank pipeline as well as our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# This load a blank pipeline in spacy model, we will model it to our needs : https://spacy.io/api/top-level\n",
    "nlp = spacy.blank(\"fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# found on https://stackoverflow.com/questions/56642816/valueerror-e024-could-not-find-an-optimal-move-to-supervise-the-parser\n",
    "\n",
    "import re\n",
    "\n",
    "def trim_entity_spans(data: list) -> list:\n",
    "    \"\"\"Removes leading and trailing white spaces from entity spans.\n",
    "\n",
    "    Args:\n",
    "        data (list): The data to be cleaned in spaCy JSON format.\n",
    "\n",
    "    Returns:\n",
    "        list: The cleaned data.\n",
    "    \"\"\"\n",
    "    invalid_span_tokens = re.compile(r'\\s')\n",
    "\n",
    "    cleaned_data = []\n",
    "    for text, annotations in data:\n",
    "        entities = annotations['entities']\n",
    "        valid_entities = []\n",
    "        for start, end, label in entities:\n",
    "            valid_start = start\n",
    "            valid_end = end\n",
    "            while valid_start < len(text) and invalid_span_tokens.match(\n",
    "                    text[valid_start]):\n",
    "                valid_start += 1\n",
    "            while valid_end > 1 and invalid_span_tokens.match(\n",
    "                    text[valid_end - 1]):\n",
    "                valid_end -= 1\n",
    "            valid_entities.append([valid_start, valid_end, label])\n",
    "        cleaned_data.append([text, {'entities': valid_entities}])\n",
    "\n",
    "    return cleaned_data\n",
    "\n",
    "TRAIN_DATAS = trim_entity_spans(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def split_dataset(dataset, train_percentage, val_percentage, seed=None):\n",
    "    \"\"\"\n",
    "    Split a dataset into training, validation, and test sets based on the provided percentages.\n",
    "    \n",
    "    Args:\n",
    "    dataset (list): A list of tuples, where each tuple contains a text and its annotations.\n",
    "    train_percentage (float): The percentage of data to be allocated for training.\n",
    "    val_percentage (float): The percentage of data to be allocated for validation.\n",
    "    seed (int) [Default: None]: Seed value for randomization.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: A tuple containing the training dataset, validation dataset, and test dataset.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    \n",
    "    random.shuffle(dataset)\n",
    "    \n",
    "    train_index = int(len(dataset) * train_percentage)\n",
    "    val_index = int(len(dataset) * (train_percentage + val_percentage))\n",
    "    \n",
    "    train_data = dataset[:train_index]\n",
    "    val_data = dataset[train_index:val_index]\n",
    "    test_data = dataset[val_index:]\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "# Example usage:\n",
    "train_data, val_data, test_data = split_dataset(TRAIN_DATAS, 0.7, 0.2)\n",
    "\n",
    "print(f\"Training data size: {len(train_data)}\")\n",
    "print(f\"Validation data size: {len(val_data)}\")\n",
    "print(f\"Test data size: {len(test_data)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will generate a train version of spacy tailored to our labels and trained to recognize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin\n",
    "from spacy.util import filter_spans\n",
    "\n",
    "# We create the document to finetune spacy model\n",
    "\n",
    "doc_bin = DocBin()  # Instantiate the DocBin class\n",
    "\n",
    "for item in val_data:\n",
    "    # print(item)\n",
    "    text = item[0]\n",
    "    labels = item[1][\"entities\"]\n",
    "    # print(labels)\n",
    "    # print(text, labels)\n",
    "    doc = nlp.make_doc(text) \n",
    "    ents = []\n",
    "    # print(labels)\n",
    "    for start, end, label in labels:\n",
    "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "        if span is None:\n",
    "            print(\"Skipping entity\")\n",
    "        else:\n",
    "            ents.append(span)\n",
    "            print(ents)\n",
    "    filtered_ents = filter_spans(ents)\n",
    "    doc.ents = filtered_ents\n",
    "    # print(doc.ents)\n",
    "    doc_bin.add(doc)\n",
    "\n",
    "doc_bin.to_disk(os.path.join(ROOT_PATH, r\"model\\unilabel\\ATOT\\val.spacy\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create the config file to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy init fill-config C:\\Users\\garsonj\\Desktop\\spacy_finetuning\\spacy_files\\train_spacy\\multilabel\\nlp_V8\\base_config.cfg C:\\Users\\garsonj\\Desktop\\spacy_finetuning\\spacy_files\\train_spacy\\multilabel\\nlp_V8\\config.cfg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train the model on our data. This operation can take several depending on the power of the computer (ideally you want to run it on you GPU if you pocess a good one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy train \"C:\\Users\\garsonj\\Desktop\\spacy_finetuning\\spacy_files\\model\\unilabel\\ATOT\\config.cfg\" --output \"C:\\Users\\garsonj\\Desktop\\spacy_finetuning\\spacy_files\\model\\unilabel\\ATOT\" --paths.train \"C:\\Users\\garsonj\\Desktop\\spacy_finetuning\\spacy_files\\model\\unilabel\\ATOT\\train.spacy\" --paths.dev \"C:\\Users\\garsonj\\Desktop\\spacy_finetuning\\spacy_files\\model\\unilabel\\ATOT\\val.spacy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy evaluate \"C:\\Users\\garsonj\\Desktop\\spacy_finetuning\\spacy_files\\model\\unilabel\\ATOT\\model-best\" \"C:\\Users\\garsonj\\Desktop\\spacy_finetuning\\spacy_files\\model\\unilabel\\ATOT\\test.spacy\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally two folder have appeared : \"model-best\" and \"model-last\". Select \"model-best\" to test it on your data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import glob\n",
    "from spacy import displacy\n",
    "\n",
    "directory = glob.glob(os.path.join(ROOT_PATH,r\"\\txt\\txt_cleaned\\*.txt\"))\n",
    "\n",
    "nlp_ner = spacy.load(os.path.join(ROOT_PATH,r\"model\\unilabel\\PPVm\\model-best\")) #select another model to test it\n",
    "\n",
    "for item in directory:\n",
    "    with open(item, \"r\", encoding = \"utf-8\") as file:\n",
    "        text = file.read()\n",
    "\n",
    "    doc = nlp_ner(text)\n",
    "    displacy.render(doc, style=\"ent\", jupyter=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "#function to write txt files from a list\n",
    "def write_txt(liste, path):\n",
    "    for i, item in enumerate(liste, start=1):\n",
    "        with open(f\"{path}\\{i}.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(item)\n",
    "\n",
    "def extract_txt_from_csv(path_csv, path_txt):\n",
    "    df = pd.read_csv(path_csv)\n",
    "    df = df[\"text\"].tolist()\n",
    "    write_txt(df, path_txt)\n",
    "\n",
    "extract_txt_from_csv(os.path.join(ROOT_PATH, r\"data\\training_csv\\data449_cats.csv\"), os.path.join(ROOT_PATH,r\"txt\\draft\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import glob\n",
    "import json\n",
    "\n",
    "\n",
    "directory = glob.glob(os.path.join(ROOT_PATH,r\"txt\\draft\\*.txt\"))\n",
    "\n",
    "nlp_ner = spacy.load(os.path.join(ROOT_PATH,r\"unilabel\\PPV_V2\\model-best\"))  # Select another model to test it\n",
    "\n",
    "# Initialize an empty list to store the output data\n",
    "output_data = []\n",
    "\n",
    "# Loop through items in the directory\n",
    "for item in directory:\n",
    "    with open(item, \"r\", encoding=\"utf-8\") as file:\n",
    "        text = file.read()\n",
    "\n",
    "    doc = nlp_ner(text)\n",
    "    \n",
    "    # Extract labeled information\n",
    "    labeled_data = [\n",
    "        {\n",
    "            \"start\": ent.start_char,\n",
    "            \"end\": ent.end_char,\n",
    "            \"labels\": ent.label_\n",
    "        }\n",
    "        for ent in doc.ents\n",
    "    ]\n",
    "\n",
    "    # Store text and labeled_data as a tuple and append to the output_data list\n",
    "    output_data.append((text, {\"label\": labeled_data}))\n",
    "\n",
    "# Save the labeled data to a JSON file\n",
    "with open(os.path.join(ROOT_PATH,r\"data\\predicted_json\\labeled_data_PPV.json\"), \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(output_data, json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(ROOT_PATH,r\"data\\predicted_json\\labeled_data_PPV.json\"), \"r\", encoding=\"utf-8\") as json_file:\n",
    "    a= json.load(json_file)\n",
    "\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy_finetuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
