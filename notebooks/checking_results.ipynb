{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this notebook is simply to compute side by side the column of different labels (AG, AI, etc.) of the true label and the predicted labels. It is manual checking mecanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_and_values(json_file, desired_label=None):\n",
    "    \"\"\"\n",
    "    Extracts text and corresponding values from a JSON file based on desired label.\n",
    "\n",
    "    Args:\n",
    "        json_file (str): Path to the JSON file.\n",
    "        desired_label (str): The label to filter (optional).\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with \"text\" and \"values\" columns.\n",
    "    \"\"\"\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    text_and_values = []\n",
    "\n",
    "    for item in data:\n",
    "        labels = item.get(\"label\", [])\n",
    "        text = item.get(\"text\")\n",
    "        values = []\n",
    "\n",
    "        for label_info in labels:\n",
    "            label_text = label_info.get(\"text\")\n",
    "            label_labels = label_info.get(\"labels\", [])\n",
    "\n",
    "            if desired_label is None or (desired_label in label_labels):\n",
    "                matches = re.findall(r'(\\d+(?:[\\.,]\\d+)?)\\s*%?', label_text)\n",
    "                values.extend([float(match.replace(',', '.').rstrip('%')) for match in matches])\n",
    "\n",
    "        if values:\n",
    "            text_and_values.append([text, values])\n",
    "\n",
    "    df = pd.DataFrame(text_and_values, columns=[\"text\", \"values\"])\n",
    "\n",
    "    #convert values to float and get rid of the list\n",
    "    df[\"values\"] = df[\"values\"].apply(lambda x: [float(value) for value in x])\n",
    "    df[\"values\"] = df[\"values\"].apply(lambda x: x[0] if len(x) == 1 else sum(x) / len(x)) # take the average if there are multiple values\n",
    "    # df[\"values\"] = df[\"values\"].apply(lambda x: x[0] if len(x) == 1 else np.median(x)) # take the median if there are multiple values\n",
    "    # df[\"values\"] = df[\"values\"].apply(lambda x: x[0] if len(x) == 1 else np.random.choice(x)) # take a random value if there are multiple values\n",
    "    df[\"values\"] = df[\"values\"].apply(lambda x: round(x, 2))\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the predicted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pred_column(df_path:str, column):\n",
    "    \"\"\"\n",
    "    Extracts text and corresponding values from a JSON file based on desired label.\n",
    "\n",
    "    Args:\n",
    "        df_path (str): Path to the DataFrame.\n",
    "        column (str): The column to extract.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(df_path)\n",
    "    df = df[[\"text\", column]]\n",
    "    return df\n",
    "\n",
    "def merge_dfs(df_true, df_pred, column):\n",
    "    \"\"\"\n",
    "    Merges two DataFrames on the text column.\n",
    "\n",
    "    Args:\n",
    "        df_true (pd.DataFrame): DataFrame with the true values.\n",
    "        df_pred (pd.DataFrame): DataFrame with the predicted values.\n",
    "        column (str): The column to rename.\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with \"text\", \"true\" and \"pred\" columns.\n",
    "    \"\"\"\n",
    "    df_true = df_true.rename(columns={\"values\": \"true\"})\n",
    "    df_pred = df_pred.rename(columns={column: \"pred\"})\n",
    "    df = pd.merge(df_true, df_pred, on=\"text\")\n",
    "    return df\n",
    "\n",
    "def clean_df(df):\n",
    "    \"\"\"\n",
    "    Takes a DataFrame and drops all rows with NaN values.\n",
    "    \"\"\"\n",
    "    df = df.dropna(subset=[\"pred\"], how=\"any\", axis=0)\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def calculate_diff(df, save_to_csv=False, save_path=None):\n",
    "    \"\"\"\n",
    "    calculates the difference between the true and predicted values and adds a column to the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with the true and predicted values.\n",
    "        save_to_csv (bool): Whether to save the DataFrame to a csv file.\n",
    "        save_path (str): Path to save the DataFrame.\n",
    "    \"\"\"\n",
    "    df[\"diff\"] = df[\"true\"] - df[\"pred\"]\n",
    "    df[\"diff\"] = df[\"diff\"].abs()\n",
    "\n",
    "    if save_to_csv:\n",
    "        df.to_csv(save_path, index=False)\n",
    "    return df\n",
    "\n",
    "def print_diff(df):\n",
    "    \"\"\"\n",
    "    Calculates the mean difference, mean true and mean predicted values and prints them.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with the true and predicted values.\n",
    "    \"\"\"\n",
    "    mean_diff = df[\"diff\"].mean()\n",
    "    mean_pred = df[\"pred\"].mean()\n",
    "    mean_true = df[\"true\"].mean()\n",
    "\n",
    "    #Calculate the mean difference and variance\n",
    "    n = len(df)\n",
    "    DN = df[\"diff\"].sum() / n\n",
    "    variance = ((df[\"diff\"] - DN) ** 2).sum() / n\n",
    "    df[\"variance\"] = variance\n",
    "\n",
    "    # var_diff = df[\"diff\"].var()\n",
    "    # var_pred = df[\"pred\"].var()\n",
    "    # var_true = df[\"true\"].var()\n",
    "    median_diff = df[\"diff\"].median()\n",
    "    median_pred = df[\"pred\"].median()\n",
    "    median_true = df[\"true\"].median()\n",
    "    # print(f\"Mean difference: {mean_diff}\")\n",
    "    # print(f\"Mean true: {mean_true}\")\n",
    "    # print(f\"Mean pred: {mean_pred}\")\n",
    "    \n",
    "    return mean_diff, mean_pred, mean_true, variance, median_diff, median_pred, median_true"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtaining the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we compare all the differences between the true and the predicted values for every label\n",
    "columns = pd.read_csv(r\"../data/processed/final_dataset.csv\").columns.tolist()\n",
    "columns.remove(\"text\")\n",
    "\n",
    "for column in columns:\n",
    "    # we extract the true and pred values from the json file and csv file\n",
    "    df_true = extract_text_and_values(r\"../data/raw/data449.json\", desired_label=column)\n",
    "    df_pred = extract_pred_column(r\"../data/processed/final_dataset.csv\", column)\n",
    "\n",
    "    # we merge the two dataframes and clean it\n",
    "    df = merge_dfs(df_true, df_pred, column)\n",
    "    df = clean_df(df) #activate or deactivate to see the difference\n",
    "\n",
    "    # we calculate the difference between the true and the predicted values and store them\n",
    "    df = calculate_diff(df, save_to_csv=True, save_path=rf\"../data/intermediate/eval_results_{column}_without_cleaning.csv\")\n",
    "    mean_diff, mean_pred, mean_true, variance, median_diff, median_pred, median_true = print_diff(df)\n",
    "\n",
    "    with open(rf\"../reports/eval_results_{column}_without_cleaning.txt\", \"w\") as f:\n",
    "        f.write(\"=====================================\\n\")\n",
    "        f.write(f\"Results for {column}\\n\")\n",
    "        f.write(\"=====================================\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write(f\"Mean difference {column}: {mean_diff}\\n\")\n",
    "        f.write(f\"Mean true {column}: {mean_true}\\n\")\n",
    "        f.write(f\"Mean pred {column}: {mean_pred}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write(f\"Variance difference {column}: {variance}\\n\")\n",
    "        # f.write(f\"Variance true {column}: {var_true}\\n\")\n",
    "        # f.write(f\"Variance pred {column}: {var_pred}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write(f\"Median difference {column}: {median_diff}\\n\")\n",
    "        f.write(f\"Median true {column}: {median_true}\\n\")\n",
    "        f.write(f\"Median pred {column}: {median_pred}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Producing a summary of the difference per label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import the different labels\n",
    "columns = pd.read_csv(r\"../data/processed/final_dataset.csv\").columns.tolist()\n",
    "columns.remove(\"text\")\n",
    "\n",
    "# We collect the mean and label and store them in a list\n",
    "means = []\n",
    "for column in columns:\n",
    "    df = pd.read_csv(rf\"../data/intermediate/eval_results_{column}_without_cleaning.csv\")\n",
    "    mean_diff = df[\"diff\"].mean()\n",
    "    means.append(mean_diff)\n",
    "\n",
    "# We create a dataframe with the labels and the mean differences\n",
    "df = pd.DataFrame(columns=[\"label\", \"mean_diff\"])\n",
    "df[\"label\"] = columns\n",
    "df[\"mean_diff\"] = means\n",
    "\n",
    "mean_diff = df[\"mean_diff\"].iloc[:7].mean() # to avoid taking in account the PPVm which value is absolute and not a percentage\n",
    "median_diff = df[\"mean_diff\"].iloc[:7].median()\n",
    "min_diff = df[\"mean_diff\"].iloc[:7].min()\n",
    "max_diff = df[\"mean_diff\"].iloc[:7].max()\n",
    "print(f\"Mean difference: {mean_diff}\")\n",
    "print(f\"Median difference: {median_diff}\")\n",
    "print(f\"Min difference: {min_diff}\")\n",
    "print(f\"Max difference: {max_diff}\")\n",
    "\n",
    "df.to_csv(r\"../data/processed/mean_diff_without_cleaning.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We count the missing results at the final stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = pd.read_csv(r\"../data/processed/final_dataset.csv\").columns.tolist()\n",
    "columns.remove(\"text\")\n",
    "\n",
    "for column in columns:\n",
    "    df = pd.read_csv(rf\"../data/intermediate/eval_results_{column}_without_cleaning.csv\")\n",
    "    full_pred_cell = df[df[\"pred\"].notna()]\n",
    "    full_true_cell = df[df[\"true\"].notna()]\n",
    "    empty_pred_cell = df[df[\"pred\"].isna()]\n",
    "\n",
    "    #We add it to the txt file\n",
    "    with open(rf\"../reports/eval_results_{column}_without_cleaning.txt\", \"a\") as f:\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"Comparison with and without cleaning\\n\")\n",
    "        f.write(f\"Full cells pred {column}: {len(full_pred_cell)}\\n\")\n",
    "        f.write(f\"Full cells true {column}: {len(full_true_cell)}\\n\")\n",
    "        f.write(f\"Empty cells {column}: {len(empty_pred_cell)}\\n\")\n",
    "\n",
    "    # print(f\"Full cells pred {column}: {len(full_pred_cell)}\")\n",
    "    # print(f\"Full cells true {column}: {len(full_true_cell)}\")\n",
    "    # print(f\"Empty cells {column}: {len(empty_pred_cell)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We merge the three data frame that summarize the mean, median and random choice efficiency approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We import the different dfs\n",
    "df_mean_cleaning = pd.read_csv(r\"../data/processed/mean_diff.csv\")\n",
    "df_mean_no_cleaning = pd.read_csv(r\"../data/processed/mean_diff_without_cleaning.csv\")\n",
    "df_mean_no_cleaning_median = pd.read_csv(r\"../data/processed/mean_diff_without_cleaning_median.csv\")\n",
    "df_mean_no_cleaning_random = pd.read_csv(r\"../data/processed/mean_diff_without_cleaning_random.csv\")\n",
    "\n",
    "# Start with the first DataFrame\n",
    "df_summary = df_mean_cleaning\n",
    "\n",
    "# Merge the other DataFrames one by one with suffixes\n",
    "dfs_to_merge = [df_mean_no_cleaning, df_mean_no_cleaning_median, df_mean_no_cleaning_random]\n",
    "list_of_suffixes = [\"_no_cleaning\", \"_no_cleaning_median\", \"_no_cleaning_random\"]\n",
    "\n",
    "for df, suffix in zip(dfs_to_merge, list_of_suffixes):\n",
    "    # Rename columns to avoid duplicates\n",
    "    columns_to_rename = {col: col + suffix for col in df.columns if col != \"label\"}\n",
    "    df = df.rename(columns=columns_to_rename)\n",
    "    df_summary = pd.merge(df_summary, df, on=\"label\")\n",
    "\n",
    "# We clean the dataframe by rounding the values\n",
    "df_summary = df_summary.apply(lambda x: round(x, 3) if x.name != \"label\" else x)\n",
    "\n",
    "# We save the dataframe\n",
    "df_summary.to_csv(r\"../data/processed/summary.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
